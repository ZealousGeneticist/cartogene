{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce517c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Take data of bioactive compounds and ask for what they interact with in homo sapiens\n",
    "#Retrieve data via CTD's batch Querry Tool, send an HTTP GET request to http://ctdbase.org/tools/batchQuery.go\n",
    "import requests, sys, json, time, math, os\n",
    "import pandas as pd\n",
    "#Input and Output File\n",
    "infile = 'bioactive.tsv'\n",
    "outfile1 = 'interactionsCTD.tsv'\n",
    "outjson = 'faceted_intact_results'\n",
    "outfile3 = 'faceted_inact_node_network.tsv'\n",
    "organism=9606\n",
    "test = True\n",
    "debug = False\n",
    "\n",
    "#Define the Program to easily request data from chems and other types of data\n",
    "def cgixns(infile, outfile1, inputType='chem', actionTypes='ANY', debug=False):\n",
    "    with open(infile, 'r') as lines:\n",
    "        inTerms = lines.read()\n",
    "    #CTD URL Batch Querry with input\n",
    "    url = 'http://ctdbase.org/tools/batchQuery.go?report=cgixns&format=tsv&inputTerms='\n",
    "    get = requests.get(url+inTerms+'&'+'inputType='+inputType+'&'+'actionTypes='+actionTypes)\n",
    "    #Save interaction data in outfile1\n",
    "    with open(outfile1, 'wb') as b:\n",
    "        b.write(get.content)\n",
    "    #Set debug=True if making/editing code\n",
    "    if debug == True:\n",
    "        print(inTerms)\n",
    "        print(type(get))\n",
    "        print(f\"{get.status_code}: {get.reason}\")\n",
    "        with open(infile, 'rb') as lines:\n",
    "            print(lines.read())\n",
    "    return print(\"Done! Have a great rest of your research! :)\")\n",
    "\n",
    "#Run\n",
    "cgixns(infile, outfile1 ,actionTypes='binding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2618ce5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define program to grab all interaction data from IntAct on all genes in CTD Data from cgixns as omniscience\n",
    "def omniscience(outfile1, outjson, jsonSize=10_000, organism=9606, test=False, debug=False):\n",
    "    if organism != 9606:\n",
    "        print('Sorry, currently only humans supported! Come back soon.')\n",
    "        return\n",
    "    ###Define function to take outfile1 dataframe and get all interactions between genes\n",
    "    #Don't need to make them connect yet with chemicals.\n",
    "    \n",
    "    ##Turn outfile1 into a dataframe with pandas\n",
    "    of1df = pd.read_table(outfile1) #outfile1 dataframe code\n",
    "    #Select for only human data (assuming human); haa stands for \"I'm only Human, After All\" (its a meme)\n",
    "    haa = of1df[of1df[\"OrganismID\"] == 9606]\n",
    "    #debug\n",
    "    if debug == True:\n",
    "        print(of1df.head(3))\n",
    "        print('\\n\\nNext Table\\n\\n')\n",
    "        print(haa.head(3))\n",
    "    \n",
    "    ##select for 5th column values, the genesymbols, and save as a list & string\n",
    "    genesymbols = haa[\"GeneSymbol\"].drop_duplicates(keep='first')  \n",
    "    gsl = genesymbols.to_list() #genesymbols list variable = gsl\n",
    "    gss = ''\n",
    "    for name in gsl:\n",
    "        if gss == '':\n",
    "            gss += name\n",
    "        else:\n",
    "                gss += ' '+name\n",
    "    #debug\n",
    "    if debug == True:\n",
    "        print(gss[0:20])\n",
    "        print(gsl[0:5])\n",
    "\n",
    "    ##Script to querry for all gene products interactions with the above bioactive compounds\n",
    "    #Search for interactions with findInteractionWithFacet on IntAct Advanced Search with gss\n",
    "    url_facet = 'https://www.ebi.ac.uk/intact/ws/interaction/findInteractionWithFacet?'\n",
    "    #The Parameters\n",
    "    pm = {\"advancedSearch\" : True, \"intraSpeciesFilter\":True, \"page\": 1, \"pageSize\": 1, \"query\":\"taxidA:9606 taxidB:9606\" + gss}\n",
    "    post = requests.post(url_facet,params=pm)\n",
    "    i = 1\n",
    "    totalele= post.json()['data']['totalElements']\n",
    "    filenum = math.ceil(totalele / jsonSize)\n",
    "    #Omniscience feedback#\n",
    "    print('The number of elements in total:',totalele)\n",
    "    del totalele\n",
    "    if test == False:\n",
    "        print(\"The number of files shall be:\",filenum)\n",
    "        print('Omniscience prepped. Beginning to write file: \\n',i,\"of\",filenum)\n",
    "    else:\n",
    "        print(\"Since this is a test, there will only be 1 file; normally, the number of files would be:\", filenum)\n",
    "        print('Omniscience prepped. Begining to write the file.')\n",
    "\n",
    "    pm['pageSize'] = jsonSize\n",
    "    #Save interactions data json in folder as outfile2\n",
    "    \n",
    "    outfile2 = outjson + str(i) + '.json'\n",
    "    print('Saving file 1...')\n",
    "    with open(outfile2, 'wb') as f:\n",
    "        for chunk in requests.post(url_facet,params=pm).iter_content(chunk_size=4096):\n",
    "            f.write(chunk)\n",
    "    print('File',i,'done. The server took',post.elapsed,'to process.\\n')\n",
    "    \n",
    "    #Option to only make 1 file to save time\n",
    "    if test == False:\n",
    "        x =  post.json()['data']['totalPages']\n",
    "        pageNum = int(x)\n",
    "        del post\n",
    "        del x\n",
    "        while i< pageNum:\n",
    "            i += 1\n",
    "            pm['page'] = i\n",
    "            #Save interactions data json in folder as outfile2\n",
    "            outfile2 = outjson + str(i) + '.json'\n",
    "            print('Assigning...')\n",
    "            with open(outfile2, 'wb') as f:\n",
    "                for chunk in requests.post(url_facet,params=pm).iter_content(chunk_size=4096):\n",
    "                    f.write(chunk)\n",
    "            print('Assigned.')\n",
    "            print('File',i,'done.\\n')\n",
    "            time.sleep(1)        \n",
    "#     #debug\n",
    "#     if debug == True:\n",
    "#         #Make a script to grab all elements (maybe after calling once and grabing elements #)\n",
    "#         numelepage = post.json()['data']['numberOfElements']\n",
    "#         print('The number of elements on page:', numelepage)\n",
    "#         pageNum =  post.json()['data']['totalPages']\n",
    "#         print('The number of pages:', pageNum)\n",
    "#         del numelepage\n",
    "#         del pageNum\n",
    "    \n",
    "    #Option to only make 1 file to save time\n",
    "#     if test == False:\n",
    "#         x =  post.json()['data']['totalPages']\n",
    "#         pageNum = int(x)\n",
    "#         del post\n",
    "#         del x\n",
    "#         while i< pageNum:\n",
    "#             i += 1\n",
    "#             pm['page'] = i\n",
    "#             #Save interactions data json in folder as outfile2\n",
    "#             outfile2 = outjson + str(i) + '.json'\n",
    "#             with open(outfile2, 'wb') as b:\n",
    "#                 print('Assigning...')\n",
    "#                 b.write(requests.post(url_facet,params=pm).content)\n",
    "#             print('Assigned.')\n",
    "#             print('File',i,'done.\\n')\n",
    "#             time.sleep(1)        \n",
    "    #Exiting Messages\n",
    "    print('Omniscience complete. \\n',i,'file(s) have been blessed upon you.')\n",
    "    print('Please consider using reductionism (the program) on your data so it is inteligible.')\n",
    "    print('The sciences shall voyage far from our island of ignorance into the midst of black seas of infinity.')\n",
    "omniscience(outfile1, outjson, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Make an edge network of source nodes and target nodes (whether chemical or gene)\n",
    "def reductionism(outfile1, outjson, outfile3, outputHeader=True, organism=9606):\n",
    "    nodesSource = []\n",
    "    nodesTarget = []\n",
    "    print('Oh yeah, its all about to be SourceNode or TargetNode up in here... \\n Please wait...')\n",
    "    ###Pull out from content moleculeA & moleculeB\n",
    "    def nodepull(b,y=[],z=[]):\n",
    "        for x in json.load(b)['data']['content']:\n",
    "            y.append(x['moleculeA'])\n",
    "            z.append(x['moleculeB'])\n",
    "    \n",
    "    #Pull up every outjson file and use for edge table\n",
    "    with os.scandir() as directory:\n",
    "        for item in directory:\n",
    "            if item.name.startswith(outjson) and item.name.endswith('.json') and item.is_file():\n",
    "                with open(item,'rb') as b:\n",
    "                    nodepull(b,nodesSource,nodesTarget) #NOTE, should replace later because opening each json is bad\n",
    "                    print(item.name, 'is done being reduced!')\n",
    "    \n",
    "    ##Put edge list for input chemicals in\n",
    "    of1df = pd.read_table(outfile1) #outfile1 dataframe code\n",
    "    #Select for only human data(assuming human); haa stands for \"I'm only Human, After All\" (its a meme)\n",
    "    haa = of1df[of1df[\"OrganismID\"] == organism]\n",
    "    ##select for 4th column values, the CASRN, and save as a list & string\n",
    "    CasRN = haa[[\"CasRN\",\"GeneSymbol\"]].drop_duplicates(keep='first')\n",
    "    for x in CasRN.CasRN.to_list():\n",
    "        nodesSource.append(x)\n",
    "    for x in CasRN.GeneSymbol.to_list():\n",
    "        nodesTarget.append(x)\n",
    "    \n",
    "    #Put Node list into node library\n",
    "    nodes = {'SourceNode':nodesSource,\n",
    "             'TargetNode':nodesTarget}\n",
    "    nodedf = pd.DataFrame(nodes)\n",
    "    ##Sort by row then by column alphanumerically to remove duplicate edges (a-b == b-a)\n",
    "    nodesort = nodedf.values\n",
    "    nodesort.sort(axis=1)\n",
    "    nodedf = pd.DataFrame(nodesort, nodedf.index, nodedf.columns)\n",
    "    nodedf = nodedf.sort_values(by='SourceNode')\n",
    "    nodedf = nodedf.drop_duplicates(keep='first')\n",
    "    #debug\n",
    "    if debug == True:\n",
    "        print(*nodes, sep='\\t')\n",
    "        print(nodedf)\n",
    "    \n",
    "    #Comes down to a 2 column dataframe in outfile3, input for outputHeader\n",
    "    nodedf.to_csv(outfile3, index=False, sep='\\t', header= outputHeader)\n",
    "    print('Reduced to atoms... or at least: \\n',os.stat(outfile3).st_size/1000, 'kB')\n",
    "reductionism(outfile1,outjson,outfile3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
